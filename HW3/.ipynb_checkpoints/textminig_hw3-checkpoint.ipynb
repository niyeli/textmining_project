{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from nltk.stem import  PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#from stop_words import get_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "line=''\n",
    "doc=[]\n",
    "term=[]\n",
    "df_dict={}\n",
    "doc_non_repeat=[]\n",
    "corpus=[]\n",
    "doc_num_tf=[]\n",
    "non_repeat_term_collect=[]\n",
    "flist = []\n",
    "for i in range(0,1095):\n",
    "    flist.append('IRTM\\\\' + str(i+1) + '.txt')\n",
    "# flist=glob.glob(r'C:\\Jupyter\\Untitled Folder\\IRTM\\*.txt') #get all the files from the d`#open each file >> tokenize the content >> and store it in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in flist:\n",
    "        tfile=open(fname,'rt', encoding='utf-8-sig')\n",
    "        line=tfile.read() # read the content of file and store in \"line\"\n",
    "        # split into words\n",
    "        tokens = word_tokenize(line,language='english')\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # stemming of words\n",
    "        porter = PorterStemmer()\n",
    "        stemmed = [porter.stem(word) for word in tokens]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in stemmed if w not in stop_words]   #words是list\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        #remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]  #此文件的term存成list\n",
    "        words = [word for word in words if len(word) >=3]\n",
    "        non_repeat_term_collect.append((set(words)))\n",
    "        doc_non_repeat=list(set(words))   #每個文件不重複的term的list\n",
    "        for i in range(len(doc_non_repeat)):\n",
    "            if doc_non_repeat[i] not in df_dict:\n",
    "                df_dict[doc_non_repeat[i]]=1\n",
    "            else:\n",
    "                df_dict[doc_non_repeat[i]]+=1\n",
    "        tf_dict={}\n",
    "#         tf_dict.append(doc_num)={}\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in tf_dict:\n",
    "                tf_dict[words[i]]=1\n",
    "            else:\n",
    "                tf_dict[words[i]]+=1\n",
    "        doc_num_tf.append(tf_dict)   #每個文件的term的數量統計\n",
    "#         doc_num+=1\n",
    "        term.append(words) #為各個文件內的文字存成list \n",
    "        str_words=\" \".join(words)   #將list轉string，以空格連接\n",
    "        #corpus.append(str_words) #一個檔案的所有term存成一個string 放到 corpus這個list裡面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(word_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=non_repeat_term_collect[0]\n",
    "for x in range(0,len(non_repeat_term_collect)):\n",
    "    yy = (yy | non_repeat_term_collect[x])  #將所有文件中的term以不重複的方式存成一個list\n",
    "union=yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laxiti',\n",
       " 'outgrowth',\n",
       " 'significantli',\n",
       " 'kemba',\n",
       " 'usvietnam',\n",
       " 'drizzli',\n",
       " 'hoteol',\n",
       " 'sponsorship',\n",
       " 'obstruct',\n",
       " 'pack',\n",
       " 'westernmad',\n",
       " 'perfectli',\n",
       " 'blunk',\n",
       " 'healthfood',\n",
       " 'bell',\n",
       " 'justic',\n",
       " 'reelect',\n",
       " 'wax',\n",
       " 'chomp',\n",
       " 'player',\n",
       " 'dell',\n",
       " 'start',\n",
       " 'still',\n",
       " 'zingic',\n",
       " 'skirmish',\n",
       " 'borivoj',\n",
       " 'fledgl',\n",
       " 'dcalif',\n",
       " 'unresolv',\n",
       " 'gutman',\n",
       " 'rebirth',\n",
       " 'veil',\n",
       " 'lida',\n",
       " 'process',\n",
       " 'notif',\n",
       " 'savior',\n",
       " 'carey',\n",
       " 'wept',\n",
       " 'prompt',\n",
       " 'bearer',\n",
       " 'termlimit',\n",
       " 'berat',\n",
       " 'flip',\n",
       " 'kuluada',\n",
       " 'public',\n",
       " 'sand',\n",
       " 'crop',\n",
       " 'candiotti',\n",
       " 'horn',\n",
       " 'financi',\n",
       " 'eccel',\n",
       " 'vat',\n",
       " 'functionari',\n",
       " 'swamp',\n",
       " 'clair',\n",
       " 'daja',\n",
       " 'provinc',\n",
       " 'arabian',\n",
       " 'luong',\n",
       " 'donald',\n",
       " 'kktv',\n",
       " 'burkhard',\n",
       " 'starr',\n",
       " 'debat',\n",
       " 'lotteri',\n",
       " 'gubernatori',\n",
       " 'chile',\n",
       " 'juryrig',\n",
       " 'pen',\n",
       " 'wilkesbarr',\n",
       " 'clement',\n",
       " 'someday',\n",
       " 'cede',\n",
       " 'theater',\n",
       " 'wear',\n",
       " 'vendor',\n",
       " 'nanci',\n",
       " 'salt',\n",
       " 'mcavoy',\n",
       " 'forgav',\n",
       " 'radov',\n",
       " 'slightli',\n",
       " 'explicitli',\n",
       " 'plaza',\n",
       " 'stuart',\n",
       " 'rout',\n",
       " 'articul',\n",
       " 'hasid',\n",
       " 'langhorn',\n",
       " 'propel',\n",
       " 'theodor',\n",
       " 'wwwgbgmumcorg',\n",
       " 'norfolk',\n",
       " 'longstand',\n",
       " 'ise',\n",
       " 'portbas',\n",
       " 'repeat',\n",
       " 'inaccur',\n",
       " 'antiballist',\n",
       " 'gave',\n",
       " 'tear',\n",
       " 'canal',\n",
       " 'cree',\n",
       " 'weekend',\n",
       " 'porterag',\n",
       " 'bion',\n",
       " 'firmli',\n",
       " 'howard',\n",
       " 'montoursvil',\n",
       " 'oklia',\n",
       " 'roug',\n",
       " 'cheer',\n",
       " 'garwood',\n",
       " 'arbadi',\n",
       " 'predat',\n",
       " 'devastatingli',\n",
       " 'hatch',\n",
       " 'fundrais',\n",
       " 'januari',\n",
       " 'fiftythre',\n",
       " 'childhood',\n",
       " 'onlin',\n",
       " 'fought',\n",
       " 'amad',\n",
       " 'hingissel',\n",
       " 'uncertainti',\n",
       " 'golf',\n",
       " 'princ',\n",
       " 'cathi',\n",
       " 'guarante',\n",
       " 'warmth',\n",
       " 'multiparti',\n",
       " 'greed',\n",
       " 'jonathan',\n",
       " 'stateown',\n",
       " 'abid',\n",
       " 'hutchinson',\n",
       " 'sop',\n",
       " 'lauri',\n",
       " 'unreason',\n",
       " 'wwwnleomfcom',\n",
       " 'talker',\n",
       " 'race',\n",
       " 'lazarevac',\n",
       " 'heal',\n",
       " 'felon',\n",
       " 'tragic',\n",
       " 'obscur',\n",
       " 'dismantl',\n",
       " 'orangereddish',\n",
       " 'rariz',\n",
       " 'armbrist',\n",
       " 'lahat',\n",
       " 'fulfil',\n",
       " 'institution',\n",
       " 'separ',\n",
       " 'memori',\n",
       " 'railroad',\n",
       " 'showdown',\n",
       " 'vest',\n",
       " 'speedskat',\n",
       " 'pedestrian',\n",
       " 'merg',\n",
       " 'attract',\n",
       " 'safeti',\n",
       " 'deplor',\n",
       " 'pent',\n",
       " 'fenc',\n",
       " 'yaakov',\n",
       " 'diari',\n",
       " 'smolder',\n",
       " 'daniela',\n",
       " 'benito',\n",
       " 'calif',\n",
       " 'betrisia',\n",
       " 'hardestfought',\n",
       " 'janata',\n",
       " 'importantli',\n",
       " 'plastic',\n",
       " 'gujarati',\n",
       " 'longabandon',\n",
       " 'grosanc',\n",
       " 'yomiuri',\n",
       " 'unscom',\n",
       " 'handler',\n",
       " 'feet',\n",
       " 'whole',\n",
       " 'delv',\n",
       " 'perufujimori',\n",
       " 'caretak',\n",
       " 'laud',\n",
       " 'ahmad',\n",
       " 'marrow',\n",
       " 'eman',\n",
       " 'address',\n",
       " 'pittman',\n",
       " 'cover',\n",
       " 'oldtim',\n",
       " 'outsmart',\n",
       " 'snowfal',\n",
       " 'gambler',\n",
       " 'solorzano',\n",
       " 'lenienc',\n",
       " 'thorni',\n",
       " 'dna',\n",
       " 'buss',\n",
       " 'cinderblock',\n",
       " 'refut',\n",
       " 'temporarili',\n",
       " 'bestknown',\n",
       " 'excerpt',\n",
       " 'sadli',\n",
       " 'outlaw',\n",
       " 'feloni',\n",
       " 'graciou',\n",
       " 'interced',\n",
       " 'kamikaz',\n",
       " 'dual',\n",
       " 'wadi',\n",
       " 'flew',\n",
       " 'search',\n",
       " 'emin',\n",
       " 'psychosi',\n",
       " 'marburi',\n",
       " 'zeri',\n",
       " 'gnassingb',\n",
       " 'incirlik',\n",
       " 'examin',\n",
       " 'gulf',\n",
       " 'entrust',\n",
       " 'rand',\n",
       " 'carlo',\n",
       " 'whop',\n",
       " 'cashstrap',\n",
       " 'carter',\n",
       " 'weird',\n",
       " 'improperli',\n",
       " 'bicycl',\n",
       " 'uphold',\n",
       " 'rental',\n",
       " 'diesel',\n",
       " 'moth',\n",
       " 'croni',\n",
       " 'water',\n",
       " 'wit',\n",
       " 'stomachchurn',\n",
       " 'thiessen',\n",
       " 'edgar',\n",
       " 'seldom',\n",
       " 'kashmir',\n",
       " 'window',\n",
       " 'burri',\n",
       " 'bore',\n",
       " 'postsoviet',\n",
       " 'outrag',\n",
       " 'youth',\n",
       " 'analyst',\n",
       " 'format',\n",
       " 'oath',\n",
       " 'redhand',\n",
       " 'ortega',\n",
       " 'substandard',\n",
       " 'diagnos',\n",
       " 'interest',\n",
       " 'mania',\n",
       " 'tekla',\n",
       " 'accommod',\n",
       " 'reson',\n",
       " 'unhappi',\n",
       " 'ascrib',\n",
       " 'kamisar',\n",
       " 'tini',\n",
       " 'earthquakeravag',\n",
       " 'francepress',\n",
       " 'salaam',\n",
       " 'wildlif',\n",
       " 'glisten',\n",
       " 'aftehock',\n",
       " 'empow',\n",
       " 'monasteri',\n",
       " 'palmsiz',\n",
       " 'quartercenturi',\n",
       " 'milit',\n",
       " 'squar',\n",
       " 'watson',\n",
       " 'purdum',\n",
       " 'oclock',\n",
       " 'stigmat',\n",
       " 'gorbachov',\n",
       " 'castil',\n",
       " 'vow',\n",
       " 'gunfir',\n",
       " 'region',\n",
       " 'exhibit',\n",
       " 'octopu',\n",
       " 'allahbad',\n",
       " 'relinquish',\n",
       " 'delus',\n",
       " 'santistevan',\n",
       " 'fremd',\n",
       " 'antidiarrhea',\n",
       " 'sometim',\n",
       " 'bushnel',\n",
       " 'graini',\n",
       " 'outtara',\n",
       " 'veget',\n",
       " 'endang',\n",
       " 'apolit',\n",
       " 'spanishspeak',\n",
       " 'withdrew',\n",
       " 'salafi',\n",
       " 'get',\n",
       " 'bout',\n",
       " 'pension',\n",
       " 'sewart',\n",
       " 'herd',\n",
       " 'henin',\n",
       " 'prosper',\n",
       " 'chandeli',\n",
       " 'ojdan',\n",
       " 'lackey',\n",
       " 'priscilla',\n",
       " 'fact',\n",
       " 'nitex',\n",
       " 'effici',\n",
       " 'envi',\n",
       " 'insati',\n",
       " 'fivestori',\n",
       " 'wreckag',\n",
       " 'muqbel',\n",
       " 'nonpartisan',\n",
       " 'ballard',\n",
       " 'earthmov',\n",
       " 'skull',\n",
       " 'kevin',\n",
       " 'otpor',\n",
       " 'ship',\n",
       " 'pariah',\n",
       " 'coretta',\n",
       " 'crumpl',\n",
       " 'vapid',\n",
       " 'dream',\n",
       " 'lumber',\n",
       " 'torricelli',\n",
       " 'murmansk',\n",
       " 'eri',\n",
       " 'mastermind',\n",
       " 'parmar',\n",
       " 'barrack',\n",
       " 'predgrag',\n",
       " 'simpl',\n",
       " 'brig',\n",
       " 'silenc',\n",
       " 'dispers',\n",
       " 'temblor',\n",
       " 'arraign',\n",
       " 'northwestern',\n",
       " 'airmen',\n",
       " 'chizov',\n",
       " 'lore',\n",
       " 'chasm',\n",
       " 'riddl',\n",
       " 'mubarak',\n",
       " 'marblehal',\n",
       " 'goingaway',\n",
       " 'commerc',\n",
       " 'lawn',\n",
       " 'touchi',\n",
       " 'hap',\n",
       " 'rodrick',\n",
       " 'better',\n",
       " 'recommend',\n",
       " 'drumbeat',\n",
       " 'despic',\n",
       " 'swore',\n",
       " 'hewitt',\n",
       " 'biograph',\n",
       " 'prestig',\n",
       " 'check',\n",
       " 'sifford',\n",
       " 'americanstatesman',\n",
       " 'serafin',\n",
       " 'zay',\n",
       " 'quakecrippl',\n",
       " 'innocu',\n",
       " 'classic',\n",
       " 'albright',\n",
       " 'severomorsk',\n",
       " 'horseback',\n",
       " 'deterior',\n",
       " 'sizabl',\n",
       " 'galapago',\n",
       " 'simpli',\n",
       " 'zogbi',\n",
       " 'serveandvolley',\n",
       " 'qaeda',\n",
       " 'tolstoy',\n",
       " 'pramod',\n",
       " 'ruano',\n",
       " 'poke',\n",
       " 'charli',\n",
       " 'valetin',\n",
       " 'bedanova',\n",
       " 'gorebush',\n",
       " 'thakur',\n",
       " 'marti',\n",
       " 'warmest',\n",
       " 'straight',\n",
       " 'excess',\n",
       " 'conven',\n",
       " 'bishop',\n",
       " 'quiet',\n",
       " 'buy',\n",
       " 'sion',\n",
       " 'courtappoint',\n",
       " 'partnership',\n",
       " 'ali',\n",
       " 'crestfallen',\n",
       " 'brightest',\n",
       " 'errrr',\n",
       " 'murray',\n",
       " 'ninepag',\n",
       " 'asa',\n",
       " 'autocrat',\n",
       " 'machin',\n",
       " 'constern',\n",
       " 'turbul',\n",
       " 'bigot',\n",
       " 'inflamm',\n",
       " 'stead',\n",
       " 'crise',\n",
       " 'evok',\n",
       " 'fidelia',\n",
       " 'afghanarab',\n",
       " 'coalfir',\n",
       " 'meantim',\n",
       " 'kafelnikov',\n",
       " 'militari',\n",
       " 'kenedi',\n",
       " 'ouattara',\n",
       " 'aok',\n",
       " 'rubber',\n",
       " 'jaysukhlalani',\n",
       " 'dug',\n",
       " 'admir',\n",
       " 'vike',\n",
       " 'longhair',\n",
       " 'defer',\n",
       " 'homicid',\n",
       " 'cannistraro',\n",
       " 'passport',\n",
       " 'willi',\n",
       " 'christen',\n",
       " 'tire',\n",
       " 'marrero',\n",
       " 'shrink',\n",
       " 'wanda',\n",
       " 'soldier',\n",
       " 'byrd',\n",
       " 'riviera',\n",
       " 'heist',\n",
       " 'perez',\n",
       " 'contin',\n",
       " 'warcrim',\n",
       " 'trace',\n",
       " 'bold',\n",
       " 'specialist',\n",
       " 'shallowwat',\n",
       " 'mighti',\n",
       " 'inquisitor',\n",
       " 'dengu',\n",
       " 'nanni',\n",
       " 'pricey',\n",
       " 'quell',\n",
       " 'weigh',\n",
       " 'rawalpindi',\n",
       " 'vienna',\n",
       " 'alhassan',\n",
       " 'cram',\n",
       " 'kaluga',\n",
       " 'similarli',\n",
       " 'marijuana',\n",
       " 'leve',\n",
       " 'bugss',\n",
       " 'twist',\n",
       " 'animos',\n",
       " 'santa',\n",
       " 'rightist',\n",
       " 'sabotag',\n",
       " 'inquisit',\n",
       " 'canon',\n",
       " 'tribal',\n",
       " 'alzawahri',\n",
       " 'assign',\n",
       " 'lace',\n",
       " 'christma',\n",
       " 'countless',\n",
       " 'oathtak',\n",
       " 'indispens',\n",
       " 'shrank',\n",
       " 'javier',\n",
       " 'munich',\n",
       " 'carper',\n",
       " 'chouchescu',\n",
       " 'pastri',\n",
       " 'israel',\n",
       " 'animist',\n",
       " 'mont',\n",
       " 'galvan',\n",
       " 'weber',\n",
       " 'illustr',\n",
       " 'wist',\n",
       " 'hasti',\n",
       " 'eas',\n",
       " 'russian',\n",
       " 'adenbas',\n",
       " 'camp',\n",
       " 'scariest',\n",
       " 'underscor',\n",
       " 'carthag',\n",
       " 'infant',\n",
       " 'exploit',\n",
       " 'unsecur',\n",
       " 'knippschild',\n",
       " 'foster',\n",
       " 'union',\n",
       " 'afford',\n",
       " 'accredit',\n",
       " 'cybernet',\n",
       " 'maganbhai',\n",
       " 'hamhand',\n",
       " 'devis',\n",
       " 'wwwinfousaidgov',\n",
       " 'reassum',\n",
       " 'montecino',\n",
       " 'elat',\n",
       " 'produc',\n",
       " 'redempt',\n",
       " 'wither',\n",
       " 'anti',\n",
       " 'forbidden',\n",
       " 'rate',\n",
       " 'wolfowitz',\n",
       " 'cornahaim',\n",
       " 'mahmood',\n",
       " 'silberl',\n",
       " 'firstround',\n",
       " 'stoko',\n",
       " 'pascual',\n",
       " 'properti',\n",
       " 'disband',\n",
       " 'impeach',\n",
       " 'molotov',\n",
       " 'lippold',\n",
       " 'crimefight',\n",
       " 'persist',\n",
       " 'surgic',\n",
       " 'late',\n",
       " 'herwig',\n",
       " 'riderless',\n",
       " 'earli',\n",
       " 'grope',\n",
       " 'halloffam',\n",
       " 'morison',\n",
       " 'outpol',\n",
       " 'impenetr',\n",
       " 'seven',\n",
       " 'straition',\n",
       " 'respons',\n",
       " 'accomplish',\n",
       " 'vero',\n",
       " 'slay',\n",
       " 'toppl',\n",
       " 'publictransport',\n",
       " 'fischer',\n",
       " 'nerv',\n",
       " 'endoftheyear',\n",
       " 'angrili',\n",
       " 'semioffici',\n",
       " 'gravi',\n",
       " 'intercontinent',\n",
       " 'mumbai',\n",
       " 'elattar',\n",
       " 'vibe',\n",
       " 'lurid',\n",
       " 'ahmadabad',\n",
       " 'babenko',\n",
       " 'thermomet',\n",
       " 'vinroot',\n",
       " 'smartli',\n",
       " 'tnt',\n",
       " 'wis',\n",
       " 'default',\n",
       " 'northern',\n",
       " 'daytona',\n",
       " 'glass',\n",
       " 'sprout',\n",
       " 'mishandl',\n",
       " 'genex',\n",
       " 'versu',\n",
       " 'studi',\n",
       " 'broke',\n",
       " 'midnight',\n",
       " 'twig',\n",
       " 'alain',\n",
       " 'wreck',\n",
       " 'speci',\n",
       " 'enigmat',\n",
       " 'insecur',\n",
       " 'everywher',\n",
       " 'zivko',\n",
       " 'ravin',\n",
       " 'anjay',\n",
       " 'jiten',\n",
       " 'gatehous',\n",
       " 'perspect',\n",
       " 'wyom',\n",
       " 'verg',\n",
       " 'schaffer',\n",
       " 'fuerth',\n",
       " 'maze',\n",
       " 'evidencegath',\n",
       " 'holiest',\n",
       " 'haphazard',\n",
       " 'anarch',\n",
       " 'justifi',\n",
       " 'experi',\n",
       " 'goddess',\n",
       " 'russianamerican',\n",
       " 'agarw',\n",
       " 'returne',\n",
       " 'pop',\n",
       " 'missourian',\n",
       " 'ponder',\n",
       " 'waifish',\n",
       " 'newli',\n",
       " 'flagdrap',\n",
       " 'flown',\n",
       " 'warrior',\n",
       " 'por',\n",
       " 'humil',\n",
       " 'medalist',\n",
       " 'costli',\n",
       " 'cochran',\n",
       " 'yank',\n",
       " 'vyacheslav',\n",
       " 'newfound',\n",
       " 'bulki',\n",
       " 'exwif',\n",
       " 'steep',\n",
       " 'foray',\n",
       " 'scout',\n",
       " 'behind',\n",
       " 'sam',\n",
       " 'categor',\n",
       " 'hien',\n",
       " 'contrit',\n",
       " 'chaplain',\n",
       " 'ungovern',\n",
       " 'crush',\n",
       " 'baffl',\n",
       " 'navyblu',\n",
       " 'clap',\n",
       " 'karita',\n",
       " 'chamberlain',\n",
       " 'dragana',\n",
       " 'fiveweek',\n",
       " 'scriptur',\n",
       " 'strickland',\n",
       " 'vituper',\n",
       " 'disbelief',\n",
       " 'legaci',\n",
       " 'therefor',\n",
       " 'cryptic',\n",
       " 'regularli',\n",
       " 'surnia',\n",
       " 'partial',\n",
       " 'bitterli',\n",
       " 'afterward',\n",
       " 'mandatori',\n",
       " 'nextofkin',\n",
       " 'clink',\n",
       " 'larg',\n",
       " 'spymast',\n",
       " 'landown',\n",
       " 'conspir',\n",
       " 'uninhabit',\n",
       " 'extend',\n",
       " 'rind',\n",
       " 'utter',\n",
       " 'gingerli',\n",
       " 'errorpron',\n",
       " 'reinforc',\n",
       " 'disillus',\n",
       " 'zivkov',\n",
       " 'wrest',\n",
       " 'nobl',\n",
       " 'season',\n",
       " 'capac',\n",
       " 'attribut',\n",
       " 'rober',\n",
       " 'height',\n",
       " 'oper',\n",
       " 'augment',\n",
       " 'prevent',\n",
       " 'irrevers',\n",
       " 'domin',\n",
       " 'hammerfest',\n",
       " 'brief',\n",
       " 'straightforward',\n",
       " 'walter',\n",
       " 'iii',\n",
       " 'uruguay',\n",
       " 'dive',\n",
       " 'pig',\n",
       " 'unforc',\n",
       " 'concert',\n",
       " 'prianka',\n",
       " 'ivorianborn',\n",
       " 'concept',\n",
       " 'taint',\n",
       " 'necessari',\n",
       " 'shebeck',\n",
       " 'grandmom',\n",
       " 'microphon',\n",
       " 'brutal',\n",
       " 'juri',\n",
       " 'wound',\n",
       " 'amic',\n",
       " 'commut',\n",
       " 'shipyard',\n",
       " 'basket',\n",
       " 'fear',\n",
       " 'preced',\n",
       " 'finit',\n",
       " 'elus',\n",
       " 'strict',\n",
       " 'alnahdi',\n",
       " 'sergaev',\n",
       " 'passeng',\n",
       " 'armitag',\n",
       " 'kingdom',\n",
       " 'farm',\n",
       " 'gritti',\n",
       " 'unionl',\n",
       " 'bare',\n",
       " 'foolish',\n",
       " 'backup',\n",
       " 'ministri',\n",
       " 'flareup',\n",
       " 'financ',\n",
       " 'koler',\n",
       " 'rabitah',\n",
       " 'firework',\n",
       " 'prospect',\n",
       " 'runup',\n",
       " 'politic',\n",
       " 'hint',\n",
       " 'bambi',\n",
       " 'threetim',\n",
       " 'ambival',\n",
       " 'stray',\n",
       " 'daze',\n",
       " 'broadbas',\n",
       " 'tsvangirai',\n",
       " 'binladen',\n",
       " 'ablaz',\n",
       " 'swissbas',\n",
       " 'typograph',\n",
       " 'lunch',\n",
       " 'john',\n",
       " 'quickli',\n",
       " 'eagleton',\n",
       " 'doubled',\n",
       " 'mozambiqu',\n",
       " 'disord',\n",
       " 'evanel',\n",
       " 'peachcolor',\n",
       " 'guido',\n",
       " 'hanoi',\n",
       " 'present',\n",
       " 'naoya',\n",
       " 'carman',\n",
       " 'careerhigh',\n",
       " 'settlement',\n",
       " 'navytown',\n",
       " 'regim',\n",
       " 'speak',\n",
       " 'russel',\n",
       " 'eighth',\n",
       " 'budva',\n",
       " 'dinh',\n",
       " 'zedek',\n",
       " 'repriev',\n",
       " 'intrus',\n",
       " 'intifada',\n",
       " 'maneuv',\n",
       " 'kumsomolitz',\n",
       " 'andi',\n",
       " 'freemarket',\n",
       " 'panick',\n",
       " 'precaut',\n",
       " 'duluth',\n",
       " 'patten',\n",
       " 'higherlevel',\n",
       " 'gruel',\n",
       " 'airlin',\n",
       " 'fell',\n",
       " 'export',\n",
       " 'lesion',\n",
       " 'explan',\n",
       " 'sachan',\n",
       " 'deter',\n",
       " 'ourselv',\n",
       " 'perhap',\n",
       " 'brainwash',\n",
       " 'instead',\n",
       " 'profession',\n",
       " 'young',\n",
       " 'elimin',\n",
       " 'courier',\n",
       " 'karadich',\n",
       " 'buff',\n",
       " 'sleight',\n",
       " 'hing',\n",
       " 'headtoto',\n",
       " 'thirdparti',\n",
       " 'reportedli',\n",
       " 'pessag',\n",
       " 'balconi',\n",
       " 'treason',\n",
       " 'harjivan',\n",
       " 'chaplin',\n",
       " 'valley',\n",
       " 'mall',\n",
       " 'stall',\n",
       " 'mathieu',\n",
       " 'excia',\n",
       " 'motorcad',\n",
       " 'investig',\n",
       " 'highli',\n",
       " 'curb',\n",
       " 'shawn',\n",
       " 'indirect',\n",
       " 'keshubhai',\n",
       " 'aussi',\n",
       " 'quarterfin',\n",
       " 'diplospeak',\n",
       " 'hernando',\n",
       " 'pontiac',\n",
       " 'dirti',\n",
       " 'jeanbaptist',\n",
       " 'runin',\n",
       " 'unjustifi',\n",
       " 'cut',\n",
       " 'harass',\n",
       " 'riversid',\n",
       " 'clearanc',\n",
       " 'facilit',\n",
       " 'regul',\n",
       " 'msnbc',\n",
       " 'incumb',\n",
       " 'hingisrazzano',\n",
       " 'maiden',\n",
       " 'iraniraq',\n",
       " 'punctur',\n",
       " 'hast',\n",
       " 'inviol',\n",
       " 'zagreb',\n",
       " 'hoss',\n",
       " 'peak',\n",
       " 'dictment',\n",
       " 'unhurt',\n",
       " 'almihdar',\n",
       " 'militarili',\n",
       " 'proceed',\n",
       " 'offshoot',\n",
       " 'outspent',\n",
       " 'sunsplash',\n",
       " 'segment',\n",
       " 'fijimori',\n",
       " 'august',\n",
       " 'flatout',\n",
       " 'refuel',\n",
       " 'succes',\n",
       " 'winton',\n",
       " 'convent',\n",
       " 'event',\n",
       " 'tax',\n",
       " 'fractur',\n",
       " 'plow',\n",
       " 'fortyseven',\n",
       " 'gerd',\n",
       " 'mydan',\n",
       " 'baghdad',\n",
       " 'kenya',\n",
       " 'faint',\n",
       " 'kristen',\n",
       " 'reestablish',\n",
       " 'equival',\n",
       " 'medicar',\n",
       " 'pli',\n",
       " 'heavili',\n",
       " 'panama',\n",
       " 'slavija',\n",
       " 'iceberg',\n",
       " 'hyena',\n",
       " 'revolv',\n",
       " 'sideway',\n",
       " 'benin',\n",
       " 'acquiesc',\n",
       " 'gerri',\n",
       " 'restiv',\n",
       " 'saba',\n",
       " 'midnovemb',\n",
       " 'glenni',\n",
       " 'alhadith',\n",
       " 'precinct',\n",
       " 'exact',\n",
       " 'gatorad',\n",
       " 'eyal',\n",
       " 'illinoi',\n",
       " 'becom',\n",
       " 'writer',\n",
       " 'children',\n",
       " 'haggard',\n",
       " 'nytimescom',\n",
       " 'seleri',\n",
       " 'head',\n",
       " 'lust',\n",
       " 'reemerg',\n",
       " 'hasten',\n",
       " 'refin',\n",
       " 'foreword',\n",
       " 'englishspeak',\n",
       " 'cal',\n",
       " 'cocktail',\n",
       " 'loudspeak',\n",
       " 'warroom',\n",
       " 'chalk',\n",
       " 'festiv',\n",
       " 'pravda',\n",
       " 'deploy',\n",
       " 'edmond',\n",
       " 'businessmen',\n",
       " 'via',\n",
       " 'plant',\n",
       " 'francisco',\n",
       " 'lightblu',\n",
       " 'glaspi',\n",
       " 'misconstru',\n",
       " 'simmon',\n",
       " 'pragu',\n",
       " 'flawless',\n",
       " 'unman',\n",
       " 'mull',\n",
       " 'carrot',\n",
       " 'hardfought',\n",
       " 'uproot',\n",
       " 'chechnya',\n",
       " 'natta',\n",
       " 'laundri',\n",
       " 'palac',\n",
       " 'fabiani',\n",
       " 'traitor',\n",
       " 'diver',\n",
       " 'lock',\n",
       " 'sammi',\n",
       " 'riowa',\n",
       " 'bring',\n",
       " 'outset',\n",
       " 'sevenyearlong',\n",
       " 'sag',\n",
       " 'script',\n",
       " 'kay',\n",
       " 'skim',\n",
       " 'fresher',\n",
       " 'convers',\n",
       " 'mladic',\n",
       " 'susan',\n",
       " 'restaur',\n",
       " 'resurg',\n",
       " 'invinc',\n",
       " 'gestur',\n",
       " 'shelv',\n",
       " 'swing',\n",
       " 'christina',\n",
       " 'vinciguerra',\n",
       " 'riad',\n",
       " 'expert',\n",
       " 'dictionari',\n",
       " 'wildli',\n",
       " 'rama',\n",
       " 'semifin',\n",
       " 'emphat',\n",
       " 'adher',\n",
       " 'almusawah',\n",
       " 'holtz',\n",
       " 'origin',\n",
       " 'chladkova',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "union=list(union)\n",
    "union.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union #all term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}  #紀錄1095個文件出現過的term的t_index編號\n",
    "fp = open(\"dictionary.txt\", \"a+\", encoding='utf-8-sig')\n",
    "for i in range(len(union)):\n",
    "    #df_dict[word] 的value是term 的df值\n",
    "        print(i+1,union[i],\"df\",df_dict[union[i]],file=fp)\n",
    "        dict[union[i]]=str(i+1) #取得這個term的編號\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in (range(1095)):\n",
    "dict_idf={}  #term 的idf值存成dict\n",
    "\n",
    "for word in dict:\n",
    "    dict_idf[word]=math.log10(1095/df_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_idf['ann']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num_tf_list=[]  #存放每個文件裡面的term字典(字典為term和出現的個數)\n",
    "#tf_idf=[]\n",
    "for i in range(len(doc_num_tf)):\n",
    "    doc_num_tf_list.append(list(doc_num_tf[i]))\n",
    "    doc_num_tf_list[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_num_tf[0]['white'] #代表第一號文件所有term的數量統計\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_0={}\n",
    "tf_idf_1={}\n",
    "\n",
    "for i in range(1095):\n",
    "    fp = open('tfidf2\\\\'+str(i+1)+\".txt\", \"a+\", encoding='utf-8')\n",
    "    #print (\"-------文件\",i+1,\"號出現的 term 和其 tf-idf unit vector------\")\n",
    "    print (\"-------文件\",i+1,\"號出現的 term 和其 tf-idf unit vector------\",file=fp) \n",
    "    tf_idf={}\n",
    "    count=0\n",
    "    for word in doc_num_tf[i]:\n",
    "        #doc_num_tf[i][doc_num_tf_list[i][j]] 為tf\n",
    "        tf_idf[word]=doc_num_tf[i][word]*(dict_idf[word])  #取出dic 的term的tf數值\n",
    "        count=count+(tf_idf[word])**2\n",
    "    count=math.sqrt(count)\n",
    "    tf_idf_list=[]\n",
    "    for word in tf_idf:\n",
    "        tf_idf[word]=tf_idf[word]/count #normalize的數值\n",
    "        if i==0:\n",
    "            tf_idf_0[word]=tf_idf[word]\n",
    "        if i==1:\n",
    "            tf_idf_1[word]=tf_idf[word]\n",
    "            \n",
    "#for i in range(len(weight)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重        \n",
    "        tf_idf_list.append(word)\n",
    "    tf_idf_list.sort()\n",
    "#     if i=0:\n",
    "#         tf_idf_0={}\n",
    "#         tf_idf_list\n",
    "    for i in range(len(tf_idf_list)):\n",
    "#         if i=1:\n",
    "#             for word in tf_idf:\n",
    "#                 if word in tf_idf_list[1]\n",
    "        dict[tf_idf_list[i]],tf_idf_list[i],tf_idf[tf_idf_list[i]]\n",
    "        print (\"t_index\",dict[tf_idf_list[i]],tf_idf_list[i],tf_idf[tf_idf_list[i]],file=fp)\n",
    "    fp.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20081144125035322\n"
     ]
    }
   ],
   "source": [
    "cosine_sim=0\n",
    "for word in tf_idf_0:  #第一號文件出現的Term\n",
    "    if word in tf_idf_1:  #如果第二號文件也有同一個term 兩者的tf-idf-unit vector 乘起來相加\n",
    "        cosine_sim=cosine_sim+(tf_idf_0[word]*tf_idf_1[word])\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term_word_feature_all[1094]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=[]   \n",
    "union2=[]\n",
    "#         line=tfile.readline() # read the content of file and store in \"line\"\n",
    "#         print(line)\n",
    "for line in open(\"classification.txt\",'rt', encoding='utf-8-sig'):\n",
    "    #print (line)\n",
    "    target_list.append(line.split(' '))\n",
    "    #將training 的classfication文件編號提出來\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=set(target_list[0])\n",
    "for i in range(len(target_list)):\n",
    "#     print(target_list[i][-1])\n",
    "    del target_list[i][0]\n",
    "    del target_list[i][-1]\n",
    "    xx = (xx | set(target_list[i]))\n",
    "xx.remove('\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xx=list(xx)\n",
    "xx.sort()\n",
    "xx = list(map(int, xx))\n",
    "dict_xx={}   #training data doc id\n",
    "for i in range(len(xx)):\n",
    "    dict_xx[xx[i]] = xx[i]  \n",
    "#將training num轉成字典\n",
    "\n",
    "testing_doc_num={}\n",
    "for i in range(1,1096):\n",
    "    if i not in dict_xx:\n",
    "            testing_doc_num[i]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=[]\n",
    "for i in dict_xx:  #to loop all the from the training data dictionary\n",
    "    zz=zz+term[dict_xx[i]-1]    #zz為all term，包括重複出現的\n",
    "#將training data doc 裡面出現過的term取出(包含重複的term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "zz=nltk.FreqDist(zz)\n",
    "word_features_dict={}\n",
    "# print(zz.most_common(500))   #計算出最多出現的前500 個term\n",
    "word_features = list(zz.keys())[:500]\n",
    "word_features.sort()\n",
    "for i in range(len(word_features)):\n",
    "    word_features_dict[word_features[i]]=int(i)  #word_feature_dict value為前500最多\n",
    "    #出現的Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將training 和testing 的data doc corpus 做處理:只留下前500個出現的term，其餘省略\n",
    "term_word_feature_all=[]\n",
    "for i in range(len(term)):\n",
    "    term_word_feature=[]\n",
    "    for j in range(len(term[i])):\n",
    "        if term[i][j] in word_features_dict:  #保存前500多個term，其餘省略掉\n",
    "            term_word_feature.append(term[i][j])\n",
    "    term_word_feature_all.append(term_word_feature)  \n",
    "    # term_word_feature_all為一個陣列，儲存每一個文件編號裡面包含的前500term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_all_term_dict={}\n",
    "for i in range(len(target_list)):\n",
    "    class_all_term=[]\n",
    "    for j in range(len(target_list[i])):\n",
    "        #target_list[0][0]為11 [0][1]為19\n",
    "        class_all_term.append(term_word_feature_all[int(target_list[i][j])-1])\n",
    "        #每一個class 包含的文件 裡面的term總數相加\n",
    "    for x in range(len(class_all_term)-1):    \n",
    "        intersection=(class_all_term[x]+class_all_term[x+1])\n",
    "    class_all_term_dict[i]=intersection\n",
    "    #一個class 出現過的term存成一個陣列，存放到字典內\n",
    "#         for k in range(len(term_word_feature_all[int(target_list[i][j])-1])):\n",
    "#         #取出第幾號文件所存放出現的500term，target_list[i][j]-1 為第幾號文件\n",
    "#             term_word_feature_all[int(target_list[i][j])-1][k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_word_probability=[]  #存放每一個class 的word_probability={}字典\n",
    "from collections import Counter\n",
    "for i in range(len(class_all_term_dict)):\n",
    "    word_probability={}  #輸入key term就可以得到term 的 probability (value)\n",
    "    x=Counter(class_all_term_dict[i])\n",
    "    for word in x:\n",
    "        word_probability[word]=(x[word]+1)/(len(class_all_term_dict[i])+500)\n",
    "        #Calculate Likelihood\n",
    "    class_word_probability.append(word_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_doc_num[17]# type(class_word_probability[0])#將每一個term的word probability列出來t\n",
    "# tetype(sting_doc_class={}\n",
    "# for) i in range(1,1096):\n",
    "#     if i not in xx:\n",
    "#         testing_doc_class[i]=i\n",
    "# type(testing_doc_num[17])\n",
    "\n",
    "# for i in testing_doc_num:\n",
    "#         for j in range(len(term_word_feature_all[i])):\n",
    "#             print(term_word_feature_all[i][j])\n",
    "#             for k in range(0,13):\n",
    "#                 term_in_class=[]\n",
    "#                 if term_word_feature_all[i][j] in class_word_probability[k]:\n",
    "#                     term_in_class.append(term_word_feature_all[i][j])\n",
    "#         #如果這個term有在第k個class的 all term字典裡面\n",
    "              #      term_in_class.append(term_word_feature_all[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_doc_class={}\n",
    "#testing_doc_num 為還未歸類的文件編號，是字典\n",
    "fp = open(\"output.txt\", \"a+\", encoding='utf-8')\n",
    "for i in testing_doc_num:\n",
    "    #print (type(i))\n",
    "    max_pp=0\n",
    "    pp=0\n",
    "    class_final=0\n",
    "    for k in range(13):\n",
    "        term_in_class=[]\n",
    "        for j in range(len(term_word_feature_all[i-1])):\n",
    "            if term_word_feature_all[i-1][j] in class_word_probability[k]:\n",
    "        #如果這個term有在第k個class的 all term字典裡面\n",
    "                term_in_class.append(term_word_feature_all[i-1][j])\n",
    "        #將他存放到term_in_class這個list裡面\n",
    "            #print(type(term_in_class[0]))\n",
    "            #print(class_word_probability[k][term_in_class[0]])\n",
    "        for x in range(len(term_in_class)-1):  #x為第k個class 有出現的term\n",
    "            pp=(class_word_probability[k][term_in_class[0]])\n",
    "            testing_term=1/13  #每一個class 的機率為1/13\n",
    "            pp=pp*(class_word_probability[k][term_in_class[x+1]])  #第幾號文本的all term(500個term)\n",
    "            pp=pp*testing_term\n",
    "            if pp> max_pp:\n",
    "                max_pp=pp\n",
    "                class_final=k\n",
    "                #print(\"x\")\n",
    "    testing_doc_class[i]=class_final+1 #testing_doc_class字典紀錄第i號文件屬於第k個class\n",
    "    \n",
    "    print (\"doc_id\",i,\"class_id\",testing_doc_class[i],file=fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_doc_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_doc_class={}\n",
    "# for i in range(1095):\n",
    "#     if i not in xx:  #testing data 第i號文件\n",
    "#         max_pp=0\n",
    "#         for k in range(13): #總共13個class\n",
    "# #             if term_word_feature_all[i][0] in class_word_probability[k] ==True:\n",
    "# #                 pp=class_word_probability[k][word_probability[term_word_feature_all[i][0]]]\n",
    "#             #第k號class 第i號文件的第j個term 的probability\n",
    "#                 for j in range(len(term_word_feature_all[i])-1):\n",
    "#                     if term_word_feature_all[i][j] in class_word_probability[k] ==True:\n",
    "#                         pp=class_word_probability[k][word_probability[term_word_feature_all[i][j]]]\n",
    "#                         testing_term=1/13  #每一個class 的機率為1/13\n",
    "#                         pp=pp* (class_word_probability[k][word_probability[term_word_feature_all[i][j+1]]])  #第幾號文本的all term(500個term)\n",
    "#                 pp=pp*testing_term\n",
    "#                 if pp> max_pp:\n",
    "#                     max_pp=pp\n",
    "#                     testing_doc_class[i+1]=k+1 #testing_doc_class字典紀錄第i號文件屬於第k個class\n",
    "#             # P(Category) * P(Word1/Category) * P(Word2/Category) * P(Word3/Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_train_num=[]\n",
    "# union2=list(xx)\n",
    "# len(union2)\n",
    "# #將training文件編號取出，按照數字大小排序\n",
    "# union2 = list(map(lambda x:int(x), union2))  #將編號sring轉為int\n",
    "# union2.sort(key=int)  #按照int排序\n",
    "# doc_train_num=union2\n",
    "\n",
    "# # del union2[0]\n",
    "# # del union2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(doc_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_test=[]\n",
    "# doc_test_num=[]\n",
    "# for i in range(1,len(corpus)+1):   #Corpus為1095個文件裡面的term 存成string\n",
    "#     if i not in doc_train_num :\n",
    "#         #將不在training 編號裡面，就把他當test 的文件\n",
    "#         doc_test.append(corpus[i-1])\n",
    "#         doc_test_num.append(i)   #test model的文件編號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
